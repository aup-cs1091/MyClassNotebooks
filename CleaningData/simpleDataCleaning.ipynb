{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code shows some simple data cleaning procedures\n",
    "\n",
    "Note that, in order to make the code simple, many details (sometimes quite important) are not considered.\n",
    "\n",
    "The input file used as example is the one generated in the TwitterFriends example\n",
    "\n",
    "\n",
    "## Main items introduced\n",
    "1. Simple data cleaning strategies\n",
    "2. Reading and writing dataframes from CSV files\n",
    "  * pd.read_csv\n",
    "  * df.to_csv\n",
    "3. Exploring dataframes\n",
    "  * df - shows beginning and end of table\n",
    "  * df.head() - shows the first five rows\n",
    "  * df\\[&lt;column_name&gt;\\] - shows the column &lt;column_name&gt;\n",
    "  * df\\[&lt;column_name&gt;\\]\\[&lt;row number&gt;\\] - shows the content of the cell\n",
    "  * df.dtypes - shows the type of each column\n",
    "  * df\\[&lt;column_name&gt;\\].unique() - shows unique values of the items in the column &lt;column_name&gt;\n",
    "  * df\\[df.&lt;column_name&gt; == '&lt;value&gt;'\\] - returns all rows that have the value &lt;value&gt; in the column &lt;column_name&gt;. Note that the condition in the square bracket may be complex, see for example code line 21\n",
    "4. Manipulating dataframes\n",
    "  * df.copy() - creates a copy of the datframe\n",
    "  * df.dropna(PARAM) - rows or columns based on NaN (no value assigned)\n",
    "  * df.drop(PARAM) - drop rows or columns\n",
    "  * df.replace(PARAM) - replaces the content of cells\n",
    "  * df\\['&lt;column_name&gt;'\\]\\[&lt;row number&gt;\\] = '&lt;value&gt;' - assignes the value &lt;value&gt; to the cell\n",
    "5. Simple regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file generated with the twitter API in a data frame and show first five records\n",
    "df = pd.read_csv('dataFromTwitter.txt', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the entire data frame - note the summary (number of row and columns) at the end\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each column can be accessed by providing its name\n",
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each element can be accessed by providing the name of the column and the row number\n",
    "df['name'][96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Every column has a type (in this case all columns contain objects)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do exercises 1,2 and 3 in the <a href=\"https://jhub.aup.edu/hub/user-redirect/git-pull?repo=https://github.com/aup-cs1091/MyClassNotebooks&branch=master&subPath=ClassExercises/CleaningDataExercises\" targwt=\"blank\">CleaningDataExercises notebook</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the data frame\n",
    "df1 = df.copy()\n",
    "# dropna allows to drop lines (or columns if axis=1) that contain non assigned values (NaN)\n",
    "# in this case I have indicated to drop the lines that have NaN in the location column (subset=['location'])\n",
    "df2 = df1.dropna(subset=['location'], axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove the status column\n",
    "df2.drop(['status'], inplace=True, axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove 4rth row which has a strange location\n",
    "df2.drop([4], inplace=True, axis=0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values in the column location\n",
    "df2['location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Washington appears in many different formats\n",
    "the lines below replace all occurences of Washington to be the same\n",
    "I have used the function replace of the pandas library (see http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html ) and a regular expression to indicate that any sequence starting by Wash and followed by any sequence of characters should be replaced by 'Washington, DC'. A good tutprial on regular expressions is at https://regexone.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.replace(to_replace=r'Wash.+',value='Washington, DC', regex=True)\n",
    "df3['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that df2 has not changed\n",
    "df2['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace any DC at the beginning of the string with Washington, DC\n",
    "# replace NYC preceded by any number of white spaces with New York, NY\n",
    "# replace a string starting and finishing with US with USA\n",
    "# replace USAðŸ‡ºðŸ‡¸ with USA\n",
    "# replace New York City with New York, NY\n",
    "# replace a string starting and finishing with New York with New York, NY\n",
    "df4=df3.replace(regex={r'^DC': 'Washington, DC', '\\s*NYC': 'New York, NY', '^US$':'USA', 'USAðŸ‡ºðŸ‡¸': 'USA', 'New York City': 'New York, NY', '^New York$':'New York, NY'})\n",
    "df4['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore which are the entities with a certain location value (e.g. which ones have USA as location)\n",
    "# (for each one of the rows in df4, [df4.location == 'USA'] returns true or false; only the rows corresponding to True are returned)\n",
    "df4[df4.location == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One location value is location, I want to see to what it corresponds\n",
    "df4[df4.location == 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I drop the line 50\n",
    "df4.drop([50], inplace=True, axis=0)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now I want to eliminate the double locations\n",
    "# From the table above I see that Reince Priebus has two locations but he also appears twice so I assign to \n",
    "# one occurence Kenosha, WI and to the other one Washington, DC\n",
    "#\n",
    "# in the column 'location' on line 6 assign 'Kenosha, WI'\n",
    "df4['location'][6] = 'Kenosha, WI'\n",
    "# in the column 'location' on line 62 assign Washington, DC'\n",
    "df4['location'][62] = 'Washington, DC'\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ann Coulter also has a double location so I check whether she also appears twice (she does not)\n",
    "df4[df4.name == 'Ann Coulter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just keep Los Angeles\n",
    "df4['location'][23] = 'Los Angeles, CA'\n",
    "df4['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for all entities with location equal 'London, Newick, LA. ' or 'London, Newick, LA.'\n",
    "df4[(df4.location == 'London, Newick, LA. ') | (df4.location == 'London, Newick, LA.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one in London and one in LA\n",
    "df4['location'][46] = 'London, UK'\n",
    "df4['location'][101] = 'Los Angeles, CA'\n",
    "df4['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last few changes\n",
    "df5=df4.replace(regex={r'United States': 'USA', '^Los Angeles$': 'Los Angeles, CA', '1600 Pennsylvania Avenue ': '1600 Pennsylvania Avenue, Washington, DC'})\n",
    "df5['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_csv('cleanDataFromTweeter.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the remaining exercises in the <a href=\"https://jhub.aup.edu/hub/user-redirect/git-pull?repo=https://github.com/aup-cs1091/MyClassNotebooks&branch=master&subPath=ClassExercises/CleaningDataExercises\" targwt=\"blank\">CleaningDataExercises notebook</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
